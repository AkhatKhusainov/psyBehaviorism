import pandas as pd
import numpy as np
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    classification_report,
    confusion_matrix
)
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_excel_results(excel_file, true_col=2, pred_col=3, sheet_name=0):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑ Excel —Ñ–∞–π–ª–∞

    Args:
        excel_file (str): –ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É
        true_col: –ò–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–∫–∏ —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2 = –∫–æ–ª–æ–Ω–∫–∞ C)
        pred_col: –ò–Ω–¥–µ–∫—Å –∫–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3 = –∫–æ–ª–æ–Ω–∫–∞ D)
        sheet_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ª–∏—Å—Ç–∞ –∏–ª–∏ –∏–Ω–¥–µ–∫—Å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0)
    """

    print(f"üìä –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞: {excel_file}")

    try:
        df = pd.read_excel(excel_file, sheet_name=sheet_name)
        print(f"‚úÖ –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ. –°—Ç—Ä–æ–∫: {len(df)}")
        print(f"üìã –ö–æ–ª–æ–Ω–∫–∏ –≤ —Ñ–∞–π–ª–µ: {list(df.columns)}")

        if isinstance(true_col, int) and isinstance(pred_col, int):
            print(f"üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º: {true_col} –∏ {pred_col}")
            y_true = df.iloc[:, true_col].values
            y_pred = df.iloc[:, pred_col].values
        else:
            print(f"üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º: '{true_col}' –∏ '{pred_col}'")
            y_true = df[true_col].values
            y_pred = df[pred_col].values

        mask = pd.notna(y_true) & pd.notna(y_pred)
        y_true = y_true[mask]
        y_pred = y_pred[mask]

        print(f"üìà –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(y_true)} –∑–∞–ø–∏—Å–µ–π (–ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö)")

        y_true = [str(cls).lower().strip() for cls in y_true]
        y_pred = [str(cls).lower().strip() for cls in y_pred]

        all_classes = sorted(list(set(y_true + y_pred)))
        print(f"üè∑Ô∏è  –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {all_classes}")

        return y_true, y_pred, all_classes, df

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
        return None, None, None, None

def calculate_detailed_metrics(y_true, y_pred, class_names):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""

    overall_accuracy = accuracy_score(y_true, y_pred)

    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred,
        labels=class_names,
        average=None,
        zero_division=0
    )

    # Accuracy –ø–æ –∫–ª–∞—Å—Å–∞–º 
    class_accuracies = []
    for class_name in class_names:
        y_true_binary = [1 if label == class_name else 0 for label in y_true]
        y_pred_binary = [1 if label == class_name else 0 for label in y_pred]

        class_acc = accuracy_score(y_true_binary, y_pred_binary)
        class_accuracies.append(class_acc)

    class_accuracies = np.array(class_accuracies)

    # –ú–∞–∫—Ä–æ-—É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    macro_precision = np.mean(precision)
    macro_recall = np.mean(recall)
    macro_f1 = np.mean(f1)
    macro_accuracy = np.mean(class_accuracies)

    # –í–∑–≤–µ—à–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='weighted', zero_division=0
    )

    return {
        'overall_accuracy': overall_accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'class_accuracies': class_accuracies,
        'support': support,
        'macro_precision': macro_precision,
        'macro_recall': macro_recall,
        'macro_f1': macro_f1,
        'macro_accuracy': macro_accuracy,
        'weighted_precision': weighted_precision,
        'weighted_recall': weighted_recall,
        'weighted_f1': weighted_f1
    }

def print_results(metrics, class_names):
    """–í—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫—Ä–∞—Å–∏–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""

    print("\n" + "="*80)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
    print("="*80)

    # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    print(f"üéØ –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Overall Accuracy): {metrics['overall_accuracy']:.4f} ({metrics['overall_accuracy']*100:.2f}%)")
    print(f"üìà –ú–∞–∫—Ä–æ F1-score: {metrics['macro_f1']:.4f}")
    print(f"üìä –ú–∞–∫—Ä–æ Accuracy: {metrics['macro_accuracy']:.4f} ({metrics['macro_accuracy']*100:.2f}%)")
    print(f"‚öñÔ∏è  –í–∑–≤–µ—à–µ–Ω–Ω—ã–π F1-score: {metrics['weighted_f1']:.4f}")

    print("\n" + "-"*80)
    print("üìã –ü–û–î–†–û–ë–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–°–ê–ú:")
    print("-"*80)

    print(f"{'–ö–ª–∞—Å—Å':<20} {'Accuracy':<10} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'–ü–æ–¥–¥–µ—Ä–∂–∫–∞':<12}")
    print("-"*80)

    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    for i, class_name in enumerate(class_names):
        print(f"{class_name:<20} {metrics['class_accuracies'][i]:<10.4f} {metrics['precision'][i]:<12.4f} "
              f"{metrics['recall'][i]:<12.4f} {metrics['f1_score'][i]:<12.4f} "
              f"{int(metrics['support'][i]):<12}")

    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    print("-"*80)
    print(f"{'–ú–∞–∫—Ä–æ —Å—Ä–µ–¥–Ω–µ–µ':<20} {metrics['macro_accuracy']:<10.4f} {metrics['macro_precision']:<12.4f} "
          f"{metrics['macro_recall']:<12.4f} {metrics['macro_f1']:<12.4f} "
          f"{int(sum(metrics['support'])):<12}")
    print(f"{'–í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ':<20} {'-':<10} {metrics['weighted_precision']:<12.4f} "
          f"{metrics['weighted_recall']:<12.4f} {metrics['weighted_f1']:<12.4f} "
          f"{int(sum(metrics['support'])):<12}")

    print("\nüí° –ü–æ—è—Å–Ω–µ–Ω–∏–µ –∫ Accuracy –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    print("   Accuracy –¥–ª—è –∫–ª–∞—Å—Å–∞ = (TP + TN) / (TP + TN + FP + FN)")
    print("   –≥–¥–µ TP/TN/FP/FN —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –¥–ª—è –∑–∞–¥–∞—á–∏ '–∫–ª–∞—Å—Å vs –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ'")
    print("   –≠—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª—å —Ä–∞–∑–ª–∏—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –æ—Ç –≤—Å–µ—Ö –¥—Ä—É–≥–∏—Ö.")

def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):
    """–°—Ç—Ä–æ–∏—Ç –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫"""

    cm = confusion_matrix(y_true, y_pred, labels=class_names)

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (Confusion Matrix)', fontsize=16, pad=20)
    plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12)
    plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=12)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"üìä –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")

    plt.show()

def save_detailed_report(metrics, class_names, y_true, y_pred, output_file):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –≤ Excel"""

    class_metrics_df = pd.DataFrame({
        '–ö–ª–∞—Å—Å': class_names,
        'Accuracy': metrics['class_accuracies'],
        'Precision': metrics['precision'],
        'Recall': metrics['recall'],
        'F1-Score': metrics['f1_score'],
        '–ü–æ–¥–¥–µ—Ä–∂–∫–∞': metrics['support']
    })

    summary_row = pd.DataFrame({
        '–ö–ª–∞—Å—Å': ['–ú–∞–∫—Ä–æ —Å—Ä–µ–¥–Ω–µ–µ', '–í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ'],
        'Accuracy': [metrics['macro_accuracy'], '-'],
        'Precision': [metrics['macro_precision'], metrics['weighted_precision']],
        'Recall': [metrics['macro_recall'], metrics['weighted_recall']],
        'F1-Score': [metrics['macro_f1'], metrics['weighted_f1']],
        '–ü–æ–¥–¥–µ—Ä–∂–∫–∞': [sum(metrics['support']), sum(metrics['support'])]
    })

    class_metrics_df = pd.concat([class_metrics_df, summary_row], ignore_index=True)


    general_metrics_df = pd.DataFrame({
        '–ú–µ—Ç—Ä–∏–∫–∞': ['–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (Overall Accuracy)', '–ú–∞–∫—Ä–æ Accuracy', '–ú–∞–∫—Ä–æ F1-Score', '–í–∑–≤–µ—à–µ–Ω–Ω—ã–π F1-Score'],
        '–ó–Ω–∞—á–µ–Ω–∏–µ': [metrics['overall_accuracy'], metrics['macro_accuracy'], metrics['macro_f1'], metrics['weighted_f1']],
        '–ü—Ä–æ—Ü–µ–Ω—Ç': [f"{metrics['overall_accuracy']*100:.2f}%",
                   f"{metrics['macro_accuracy']*100:.2f}%",
                   f"{metrics['macro_f1']*100:.2f}%",
                   f"{metrics['weighted_f1']*100:.2f}%"]
    })


    cm = confusion_matrix(y_true, y_pred, labels=class_names)
    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)

    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        general_metrics_df.to_excel(writer, sheet_name='–û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏', index=False)
        class_metrics_df.to_excel(writer, sheet_name='–ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º', index=False)
        cm_df.to_excel(writer, sheet_name='–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')

    print(f"üíæ –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    print("üöÄ –ê–ù–ê–õ–ò–ó–ê–¢–û–† –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
    print("="*50)


    excel_file = input("üìÅ –í–≤–µ–¥–∏—Ç–µ –ø—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É: ").strip()

    print("\n–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∏ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ 2 –∏ 3 (C –∏ D –≤ Excel)")
    custom_cols = input("–•–æ—Ç–∏—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å? (y/n): ").strip().lower()

    true_col = 2 
    pred_col = 3  

    if custom_cols == 'y':
        print("–í–≤–µ–¥–∏—Ç–µ –∏–Ω–¥–µ–∫—Å—ã –∫–æ–ª–æ–Ω–æ–∫ (A=0, B=1, C=2, D=3, –∏ —Ç.–¥.)")
        print("–ò–ª–∏ –≤–≤–µ–¥–∏—Ç–µ —Ç–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –∏–∑ —Ñ–∞–π–ª–∞")

        true_input = input("–ò—Å—Ç–∏–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–∫–æ–ª–æ–Ω–∫–∞): ").strip()
        pred_input = input("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã (–∫–æ–ª–æ–Ω–∫–∞): ").strip()

        try:
            true_col = int(true_input)
        except ValueError:
            true_col = true_input

        try:
            pred_col = int(pred_input)
        except ValueError:
            pred_col = pred_input

    y_true, y_pred, class_names, df = analyze_excel_results(excel_file, true_col, pred_col)

    if y_true is None:
        return

    metrics = calculate_detailed_metrics(y_true, y_pred, class_names)

    print_results(metrics, class_names)

    print("\n" + "="*50)
    print("–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –û–ü–¶–ò–ò:")

    show_cm = input("üìä –ü–æ–∫–∞–∑–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫? (y/n): ").strip().lower()
    if show_cm == 'y':
        plot_confusion_matrix(y_true, y_pred, class_names)

    save_report = input("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –≤ Excel? (y/n): ").strip().lower()
    if save_report == 'y':
        output_file = input("–í–≤–µ–¥–∏—Ç–µ –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, report.xlsx): ").strip()
        if not output_file.endswith('.xlsx'):
            output_file += '.xlsx'
        save_detailed_report(metrics, class_names, y_true, y_pred, output_file)

    show_sklearn = input("üìã –ü–æ–∫–∞–∑–∞—Ç—å sklearn classification_report? (y/n): ").strip().lower()
    if show_sklearn == 'y':
        print("\n" + "="*50)
        print("SKLEARN CLASSIFICATION REPORT:")
        print("="*50)
        print(classification_report(y_true, y_pred))

if __name__ == "__main__":
    main()
